{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#基于spark中ALS的推荐系统，针对movielens中电影打分数据做推荐\n",
    "#Edit: 寒小阳（hanxiaoyang.ml@gmail.com）\n",
    "import sys\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join,isfile,dirname\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "def parseRating(line):\n",
    "    \"\"\"\n",
    "    Movielens的打分格式是 userId::movielenId::rating::timestamp\n",
    "    我们对格式做一个解析\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return long(fields[3]) % 10,(int(fields[0]),int(fields[1]),float(fields[2]))  # long(fields[3])是什么意思？？？\n",
    "\n",
    "def parseMovie(line):\n",
    "    \"\"\"\n",
    "    对应的电影文件的格式为movieId::movieTitle\n",
    "    解析成int id,文本\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return int(fields[0]),fields[1]\n",
    "\n",
    "def loadRatings(ratingsFile):\n",
    "    \"\"\"\n",
    "    载入得分\n",
    "    \"\"\"\n",
    "    if not isfile(ratingsFile):\n",
    "        print(\"File %s dose not exist\" % ratingFile)\n",
    "        sys.exit(1)\n",
    "    f = open(ratingFile,'r')\n",
    "    ratings = filter(lambda r:r[2] > 0,[parseRating(line)[1] for line in f]) #把评分》0的sample取出来\n",
    "    f.close()\n",
    "    if not ratings:\n",
    "        print(\"No ratings provided\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return ratings\n",
    "    \n",
    "def computeRmse(model,data,n):\n",
    "    \"\"\"\n",
    "    评估的时候要用的，计算均方根误差\n",
    "    \"\"\"\n",
    "    predictions = model.predictAll(data,map(lambda x:(x[0],x[1]))) #predict user,item rating\n",
    "    predictionsAndRatings = predictions.map(lambda x:((x[0],x[1]),x[2])) \\\n",
    "        .join(data.map(lambda x: ((x[0],x[1]),x[2]))) \\\n",
    "        .values() #(user,item,prediction,true)\n",
    "    return sqrt(predictionsAndRatings.map(lambda x:(x[0] - x[1]) ** 2).reduce(add) /float(len(data)))  #这里的map，reduce就是并行，聚合？？？\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if (len(sys.argv) != 3):\n",
    "        print(\"Usage: /path/to/spark/bin/spark-submit --driver-memory 2g\" + \\\n",
    "             \"MovieLensALS.py movieLensDataDir personalRatingsFile\")\n",
    "        sys.exit(1)\n",
    "    #设定环境\n",
    "    conf = SparkConf().setAppName(\"MovieLensALS\").set(\"spark.executor.memory\",\"2g\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    \n",
    "    #载入打分数据\n",
    "    myRatings = loadRatings(sys.argv[2])\n",
    "    myRatingsRDD = sc.parallelize(myRatings,1) #转为RDD，partition=1\n",
    "    \n",
    "    movieLensHomeDir = sys.argv[1]\n",
    "    \n",
    "    #得到的ratings为（时间戳最后一位整数，(userId,movieId,rating）格式的RDD\n",
    "    ratings = sc.textFile(join(movieLensHomeDir,\"ratings.dat\")).map(parseRating)\n",
    "    \n",
    "    #得到的movies为（movieId,movieTitle）格式的RDD\n",
    "    movies = sc.textFile(join(movieLensHomeDir,\"ratings.dat\")).map(parseMovie).collect() #id,movie ;  reduce() 和 collect()有什么区别\n",
    "    \n",
    "    numRatings = ratings.count()\n",
    "    numUsers = ratings.values().map(lambda r: r[0]).distinct().count()\n",
    "    numMovies = ratings.values().map(lambda r: r[1]).distinct().count()\n",
    "    \n",
    "    print(\"Got %d ratings from %d users on %d movies\" % (numRatings,numUsers,numMovies))\n",
    "    \n",
    "    #根据时间戳最后一位把整个数据集分成训练集（60%），交叉验证集（20%），和评估集（20%）\n",
    "    \n",
    "    #训练，交叉验证，测试集 都是 (userId,movieId,rating)格式的RDD\n",
    "    \n",
    "    numPartitions = 4 #并行运算\n",
    "    training = ratings.filter(lambda x: x[0] < 6)\\\n",
    "        .values() \\ \n",
    "        .union(myRatingsRDD) \\ \n",
    "        .repartition(numPartitions) #分成4个partitions\n",
    "        .cache()\n",
    "        \n",
    "    validation = ratings.filter(lambda x: x[0] >= 6 and x[0] < 8) \\\n",
    "        .values() \\\n",
    "        .repartition(numPartitions) \\\n",
    "        .cache()\n",
    "    \n",
    "    test = ratings.filter(lambda x: x[0] >= 8).values().cache()\n",
    "    \n",
    "    numTraining = training.count()\n",
    "    numValidation = validation.count()\n",
    "    numTest = test.count()\n",
    "    \n",
    "    print(\"Training: %d validation: %d test: %d\" % (numTraining,numValidation,numTest))\n",
    "    \n",
    "    #训练模型，在交叉验证集上看效果\n",
    "    \n",
    "    ranks = [8,12]\n",
    "    lambdas = [0.1,10.0]\n",
    "    numIters = [10,20]\n",
    "    bestModel = None\n",
    "    bestValidationRmse = float(\"inf\")\n",
    "    bestRank = 0\n",
    "    bestLambda = -1.0\n",
    "    bestNumIters = -1\n",
    "    \n",
    "    for rank,lmbda,numIter in itertools.product(ranks,lambdas,numIters):\n",
    "        model = ALS.train(training,rank,numIter,lmbda)\n",
    "        validationRmse = computeRmse(model,validation,numValidation)\n",
    "        print(\"RMSE (validation) = %f for the model trained with\" % validationRmse + \\\n",
    "             \"rank = %d, lambda = %.1f, and numIter = %d\" % (rank,lmbda,numIter))\n",
    "        if(validationRmse < bestValidationRmse):\n",
    "            bestModel = model\n",
    "            bestValidationRmse = validationRmse\n",
    "            bestRank = rank\n",
    "            bestLambda = lmbda\n",
    "            bestNumIter = numIter\n",
    "    testRmse = computeRmse(bestModel,test,numTest)\n",
    "    \n",
    "    #在测试集上评估 交叉验证集上最好的模型\n",
    "    print(\"the best model was trained with rank = %d and lambda = %.1f,\" % (bestRank,bestLambda) \\\n",
    "         + \"and numIter = %d, and its RMSE on the test set is %f\" % (bestNumIter,testRmse))\n",
    "    \n",
    "    #我们把基线模型设定为每次都返回平均得分的模型\n",
    "    meanRating = training.union(validation).map(lambda x:x[2]).mean()\n",
    "    baselineRmse = sqrt(test.map(lambda x:(meanRating - x[2]) ** 2).reduce(add) / numTest)\n",
    "    improvement = (baselineRmse - testRmse) / baselineRmse * 100\n",
    "    print(\"the best model improves the baseline by %2f\" % (improvement) + \"%.\")\n",
    "    \n",
    "    #个性化的推荐（针对某个用户）\n",
    "    myRatedMovieIds = set([x[1] for x in myRatings]) #针对的是x[0]的user\n",
    "    candidates = sc.parallelize([m for m in movies if m not in myRateMovieIds]) #将那些不在已评价movie中的movie取出来；转为RDD数据\n",
    "    predictions = bestModel.predictAll(candidates.map(lambda x: (0,x))).collect() #这里的0应该是指userid\n",
    "    recommendations = sorted(predictions,key=lambda x: x[2],reverse=True)[:50]\n",
    "    \n",
    "    print(\"Movies recommended for you:\")\n",
    "    for i in xrange(len(recommendations)):\n",
    "        print(\"%2d:%s\" % (i+1,movies[recommendations[i][1]])).encode(\"ascii\",'ignore')\n",
    "    \n",
    "    #clean up \n",
    "    sc.stop()\n",
    "    \n",
    "    \n",
    "    #collect()和reduce()的区别？？？？\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
