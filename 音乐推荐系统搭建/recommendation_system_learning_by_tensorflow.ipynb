{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LFM构建推荐系统\n",
    "y_pred[u,i] = bias_global + bias_user[u] + bias_item[i] + <embedding_user[u],embedding_item[i]>\n",
    "loss function:\n",
    "    |y_pred[u,i] - y_true[u,i]|**2 + lambda(|embedding_user[u]|**2 + |embedding_item[i]|**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据\n",
    "以movielens为例，数据格式为：user item rating timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这部分代码大家不用跑，因为数据已经下载好了\n",
    "#!wget http://files.grouplens.org/datasets/movielens/ml-lm.zip\n",
    "#!sudo unzip ml-lm zip -d ./movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理部分\n",
    "tensorflow 搭建的模型，训练方式通常是 一个batch一个batch训练的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-cb7811d02f8b>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-cb7811d02f8b>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    df[\"rate\"] = df[]\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from _future_ import absolute_import,division,print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#读取数据\n",
    "def read_data_and_process(filename,sep=\"\\t\"):\n",
    "    col_names = [\"user\",\"item\",\"rate\",\"st\"]\n",
    "    df = pd.read_csv(filename,sep=sep,header=None,names=col_names,engine=\"python\")\n",
    "    df[\"user\"] -= 1 #id号从0开始\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\",\"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练用的数据，最好shuffle，防止一个batch清一色\n",
    "class ShuffleDataIterator(object):\n",
    "    \"\"\"\n",
    "    随机生成一个batch一个batch的数据\n",
    "    \"\"\"\n",
    "    #初始化\n",
    "    def __init__(self,inputs,batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.len)]))\n",
    "\n",
    "    #总样本量\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    #取出下一个batch\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    #随机生成batch_size个下标，取出对应的样本\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0,self.len,(self.batch_size,))\n",
    "        out = self.inputs[ids,:] \n",
    "        return [out[:,i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试用的数据，不需要shuffle\n",
    "class OneEpochDataIterator(ShuffleDataIterator):\n",
    "    \"\"\"\n",
    "    顺序产出一个epoch的数据，在测试中可能会用到\n",
    "    \"\"\"\n",
    "    def __init__(self,inputs,batch_size = 10):\n",
    "        super(OneEncoderDataIterator,self).__init__(inputs,batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len),np.ceil(self.len/batch_size)) #将index分为 self.len/batch_size份\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "    \n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id],:]\n",
    "        self.group_id += 1\n",
    "        return [out[:,i] for i in range(len(self.num_cols))]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#使用矩阵分解搭建的网络结构\n",
    "def inference_svd(user_batch,item_batch,user_num,item_num,dim=5,device=\"/cpu:0\"):\n",
    "    #使用CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        #初始化几个bias项\n",
    "        global_bias = tf.get_variable(\"global_bias\",shape=[]) #???\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\",shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\",shape=[item_num])\n",
    "        #bias向量\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user,user_batch,name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item,item_batch,name=\"bias_item\")\n",
    "        w_user = tf.get_variable(\"embd_user\",shape=[user_num,dim],initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\",shape=[item_num,dim],initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        #user向量与item向量\n",
    "        embd_user = tf.nn.embedding_lookup(w_user,user_batch,name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item,item_batch,name=\"embedding_item\")#???\n",
    "    with tf.device(device):\n",
    "        #按照实际公式进行计算\n",
    "        #先对user向量和Item向量求内积\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user,embd_item),1)\n",
    "        #加上几个偏置项\n",
    "        infer = tf.add(infer,global_bias)\n",
    "        infer = tf.add(infer,bias_user)\n",
    "        infer = tf.add(infer,bias_item,name=\"svd_inference\")\n",
    "        #加上正则化项\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user),tf.nn.l2_loss(embd_item),name=\"svd_inference\")\n",
    "        return infer,regularizer\n",
    "\n",
    "#迭代优化部分\n",
    "def optimization(infer,regularizer,rate_batch,learning_rate=0.001,reg=0.1,device=\"/cpu:0\"):\n",
    "    global_step=tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "    #选择合适的optimizer做优化\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer,rate_batch))\n",
    "        penalty = tf.constant(reg,dtype=tf.float32,shape=[],name=\"l2\")\n",
    "        cost = tf.add(cost_l2,tf.multiply(regularizer,penalty))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step) #优化器返回的是什么？？？\n",
    "        return cost,train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据上的模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "\n",
    "np.random.seed(13575)\n",
    "#一批数据的大小\n",
    "BATCH_SIZE=1000\n",
    "#用户数\n",
    "USER_NUM=6040\n",
    "#电影数\n",
    "ITEM_NUM = 3952\n",
    "#factor维度\n",
    "DIM = 15\n",
    "#最大迭代轮数\n",
    "EPOCH_MAX = 100\n",
    "#使用cpu的训练\n",
    "DEVICE = \"/cpu:0\"\n",
    "\n",
    "#截断\n",
    "def clip(x): #如果x小于1或大于5，截断为1或5，否则为x\n",
    "    return np.clip(x,1.0,5.0)\n",
    "\n",
    "#这是方便tensorflow可视化做的summary\n",
    "def make_scalar_summary(name,val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name,simple_value=val)])\n",
    "\n",
    "#调用上面的函数获取数据\n",
    "def get_data():\n",
    "    df = read_data_and_process(\"./movielens/ml-lm/ratings.dat\",sep=\"::\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * 0.9)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    print(df_train.shape,df_test.shape)\n",
    "    return df_train,df_test\n",
    "\n",
    "#实际训练过程\n",
    "def svd(train,test):\n",
    "    samples_per_batch = len(train)/BATCH_SIZE\n",
    "    #一批一批数据用于训练\n",
    "    iter_train = ShuffleDataIterator([train[\"user\"],train[\"item\"],train[\"rate\"]],batch_size=BATCH_SIZE)\n",
    "    #测试数据\n",
    "    iter_test = OneEpochDataIterator([test[\"user\"],test[\"item\"],test[\"rate\"]],batch_size=-1)\n",
    "    #user和item batch\n",
    "    user_batch = tf.placeholder(tf.int32,shape=[None],name=\"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32,shape=[None],name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float32,shape=[None])\n",
    "    \n",
    "    #构建graph和训练\n",
    "    infer,regularizer = inference_svd(user_batch,item_batch,user_num=USER_NUM,item_num=ITEM_NUM,dim=DIM,device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _,train_op = optimization(infer,regularizer,rate_batch,learning_rate=0.001,reg=0.1,device=DEVICE)\n",
    "    \n",
    "    #初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    #开始迭代\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"/tmp/svd/log\",graph=sess.graph)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\",\"train_error\",\"val_error\",\"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch) #???\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch): #???抽取样本的次数为：EPOCH_MAX * samples_per_batch ???\n",
    "            users,items,rates = next(iter_train)\n",
    "            _,pred_batch = sess.run([train_op,infer],feed_dict = {user_batch:users,item_batch:items,rate_batch_rates})\n",
    "            pred_batch = clip(pred_batch) #???pred_batch是什么东东？？？\n",
    "            errors.append(np.power(pred_batch - rates,2)) #训练误差\n",
    "            if i % samples_per_batch = 0:\n",
    "                train_err = np.sqrt(np.mean(errors)) #一批数据的误差\n",
    "                test_err2 = np.array([])\n",
    "                for users,items,rates in iter_test:\n",
    "                    pred_batch = sess.run(infer,feed_dict = {user_batch:users,item_batch:items}) #有个疑问，在tensorflow中训练出的模型怎么保存，怎么导入？？？\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2,np.power(pred_batch - rates,2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print(\"{:3d} {:f} {:f}(s)\".format(i // samples_per_batch,train_err,end-start))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\",train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\",test_err)\n",
    "                summary_writer.add_summary(train_err_summary,i)\n",
    "                summary_writer.add_summary(test_err_summary,i)\n",
    "                start = end\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据\n",
    "df_train,df_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#完成实际的训练\n",
    "svd(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
