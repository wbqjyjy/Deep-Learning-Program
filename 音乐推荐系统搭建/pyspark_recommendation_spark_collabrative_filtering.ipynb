{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果数据量很大，需要用spark分布式平台来完成协同过滤的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-based协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf8 -*-\n",
    "#pySpark实现的基于用户的协同过滤\n",
    "#使用的是余弦相似度\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def parseVectorOnUser(line):\n",
    "    \"\"\"\n",
    "    解析数据，Key是user，后面是item和rate\n",
    "    \"\"\"\n",
    "    line = line.split(\"|\")\n",
    "    return line[0],(line[1],float(line[2]))\n",
    "\n",
    "def parseVectorOnItem(line): #line : (user,item,rate)\n",
    "    \"\"\"\n",
    "    解析数据，key是item，后面是user和打分\n",
    "    \"\"\"\n",
    "    line = line.split(\"|\")\n",
    "    return line[1],(line[0],float(line[2]))\n",
    "\n",
    "def sampleInteractions(item_id,users_with_rating,n):\n",
    "    \"\"\"\n",
    "    如果某个商品上用户行为特别多，可以选择适当做点下采样\n",
    "    \"\"\"\n",
    "    if len(users_with_rating) > n:\n",
    "        return item_id,random.sample(users_with_rating,n)\n",
    "    else:\n",
    "        return item_id,users_with_rating\n",
    "    \n",
    "def findUserPairs(item_id,users_with_rating):\n",
    "    \"\"\"\n",
    "    对每个Item,找到共同打分的user对\n",
    "    \"\"\"\n",
    "    for user1,user2 in combinations(users_with_rating,2):\n",
    "        return (user1[0],user2[0]),(user1[1],user2[1])\n",
    "    \n",
    "def calcSim(user_pair,rating_pairs):\n",
    "    \"\"\"\n",
    "    对每个user对，根据打分计算余弦距离，并返回共同打分的item个数\n",
    "    \"\"\"\n",
    "    sum_xx,sum_xy,sum_yy,sum_x,sum_y,n = (0.0,0.0,0.0,0.0,0.0,0)\n",
    "    for rating_pair in rating_pairs: #每一个循环为一个共同打分item\n",
    "        sum_xx += np.float(rating_pair[0]) * np.float(rating_pair[0])\n",
    "        sum_yy += np.float(rating_pair[1]) * np.float(rating_pair[1])\n",
    "        sum_xy += np.float(rating_pair[0]) * np.float(rating_pair[1])\n",
    "        n += 1\n",
    "    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))\n",
    "    return user_pair,(cos_sim,n) #???什么鬼输出？？？  不是该user_pair和cos_sim一一对应吗？\n",
    "\n",
    "def cosine(dot_product,rating_norm_squared,rating2_norm_squared):\n",
    "    \"\"\"\n",
    "    2个向量A和B的余弦相似度\n",
    "    dotProduct(A,B) / (norm(A) * norm(B))\n",
    "    \"\"\"\n",
    "    numerator = dot_product\n",
    "    denominator = rating_norm_squared * rating_norm_squared\n",
    "    return (numerator / (float(denominator))) if denominator else 0.0\n",
    "\n",
    "def keyOnFirstUser(user_pair,item_sim_data):\n",
    "    \"\"\"\n",
    "    对于每个user-user对，用第一个user做key\n",
    "    \"\"\"\n",
    "    (user1_id,user2_id) = user_pair\n",
    "    return user1_id,(user2_id,item_sim_data)\n",
    "\n",
    "def nearestNeighbors(user,users_and_sims,n):#user是一个还是一对？ users_and_sims的形式：user_pari,(cosin_sim,n)\n",
    "    \"\"\"\n",
    "    选出相似度最高的N个邻居\n",
    "    \"\"\"\n",
    "    users_and_sims.sort(key=lambda x:x[1][0],reverse=True)  #逆序排\n",
    "    return user,user_and_sims[:n] #返回该user的前n个相似的user\n",
    "\n",
    "def topNRecommendations(user_id,user_sims,users_with_rating,n):#(user1_id,((user2,(cos_sim,n)),...),(users:(item,rating)),n)\n",
    "    \"\"\"\n",
    "    根据最近的N个邻居进行推荐\n",
    "    \"\"\"\n",
    "    totals = defaultdict(int)\n",
    "    sim_sums = defaultdict(int)\n",
    " #for(pre_item,rating) in user_with_rating.get(user_id,None):\n",
    "#   if rating != Null:\n",
    "#       continue\n",
    "     \n",
    "    for (neighbor,(sim,count)) in user_sims: #neighbor为与user_id相近的邻居\n",
    "        #遍历邻居的打分\n",
    "        unscored_items = users_with_rating.get(neighbor,None) #得到该neighbor的(item，rating)\n",
    "        if unscored_items:\n",
    "            for (item,rating) in unscored_items:#？？？\n",
    "                if neighbor != item: # if pre_item == item\n",
    "                    #更新推荐度和相似度\n",
    "                    totals[neighbor] += sim * rating #totals[pre_item] += sim * rating\n",
    "                    sim_sums[neighbor] += sim \n",
    "    \n",
    "    #归一化\n",
    "    scored_items = [(total/sim_sums[item],item) for item,total in totals.items()] #???totals.items()返回的是（Key，value）\n",
    "    #按照推荐度降序排列\n",
    "    scored_items.sort(reverse=True)\n",
    "    #推荐度的item\n",
    "    ranked_items = [x[1] for x in scored_items]\n",
    "    return user_id,ranked_items[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 3:\n",
    "        print(sys.stderr,\"Usage:PythonUserCF <master> <file>\")\n",
    "        exit(-1)\n",
    "    sc = SparkContext(sys.argv[1],\"PythonUserCF\") #没有设定SparkConf()???\n",
    "    lines = sc.textFile(sys.argv[2]) #读文件\n",
    "    \"\"\"\n",
    "    处理数据，获得稀疏的item-user矩阵\n",
    "    item_id -> ((user_1,rating),(user2,rating))\n",
    "    \"\"\"\n",
    "    item_user_pairs = lines.map(parseVectorOnItem).groupByKey().map( #{item:(user,rate)}\n",
    "    lambad p:sampleInteractions(p[0],p[1],500)).cache()#itemid,user_with_rating,n 返回 （item_id,(user,rate)）\n",
    "    \"\"\"\n",
    "    获得2个用户所有的item-item对得分组合：\n",
    "    (user1_Id,user2_id) -> [(rating1,rating2),\n",
    "                            (rating1,rating2),\n",
    "                            (rating1,rating2),\n",
    "                            ...]\n",
    "    \"\"\"\n",
    "    pairwise_users = item_user_pairs.filter(#groupByKey()之前是 ((user1,user2),(rate1,rate2));groupByKey()后，key：（user1,user2）,value:(rate1,rate2)\n",
    "        lambda p:len(p[1]) > 1).map(\n",
    "        lambda p: findUserPairs(p[0],p[1])).groupByKey() #p[0]:item_id,p[1]:(user,rate) 返回：((user1,user2),(rate1,rate2))对同一item的2个user的打分\n",
    "    \"\"\"\n",
    "    计算余弦相似度，找到最近的N个邻居：\n",
    "    (user1,user2) -> (similarity,co_raters_count)\n",
    "    \"\"\"\n",
    "    user_sims = pairwise_users.map( #pairwise_users: ((user1,user2):((rate1,rate2),(...))\n",
    "        lambda p:calcSim(p[0],p[1])).map( #(user_pair,rating_pair) 返回的是(user_pair,(cos_sim,n))\n",
    "        lambda p:keyOnFirstUser(p[0],p[1])).groupByKey().map( #(user_pair,item_sim_data)返回的是（user1_id,(user2_id,(cos_sim,n)）,groupbykey()以后，每个user1_id，的相似的(user2_id,(cos_sim,n))\n",
    "        lambda p:nearestNeighbors(p[0],p[1],50)) #(user1_id,((user2_id,(cos_sim,n)),...),n)，返回(user1_Id,((user2_id,(cos_sim,n)),(...),(...)))\n",
    "    \"\"\"\n",
    "    对每个用户的打分记录整理成如下形式：\n",
    "    user_id -> [(item_id_1,rating_1),\n",
    "                 (item_id_2,rating_2),\n",
    "                 ...]\n",
    "    \"\"\"\n",
    "    user_item_hist = lines.map(parseVectorOnUser).groupByKey().collect() #(user,((item,rate),(...),(...)))\n",
    "    ui_dict = {}\n",
    "    for (user,items) in user_item_hist:\n",
    "        ui_dict[user]=items\n",
    "    uib = sc.broadcast(ui_dict) #把文件发送到其它结点？？？\n",
    "    user_item_recs = user_sims.map(lambda p: topNRecommendations(p[0],p[1],uib.value,100)) #(user1_id,({user2_id:(cos_sim,n),{user3_id,(cos_sim,n)},...},((item,rating),(item,rating)),100))\n",
    "                                                             #(p[0],p[1],uib,100)\n",
    "                                                             #map().reduce()???\n",
    "    #？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上边的操作都是只有map，没有reduce啊？？？\n",
    "#reduce()是只用一次就可以吗？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
